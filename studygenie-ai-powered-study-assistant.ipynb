{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üìö StudyGenie: AI-Powered Study Assistant\n\n## Project Overview\n\n**StudyGenie** is an innovative AI-powered study assistant designed to transform how students engage with academic materials. By leveraging advanced generative AI capabilities, StudyGenie processes uploaded PDF documents, answers questions with context-aware precision, and generates tailored study resources like summaries, multiple-choice questions (MCQs), and flashcards. This project addresses a real-world problem: the challenge students face in efficiently studying complex materials, retaining key concepts, and preparing for exams.\n\n### Why StudyGenie?\n- **Real-World Impact**: Helps students save time, understand complex topics, and excel in their studies.\n- **Innovative Use Case**: Combines document understanding, retrieval-augmented generation (RAG), and structured output to create a seamless learning experience.\n- **Gen AI Capabilities**:\n  - **Document Understanding**: Extracts and processes text from PDF documents.\n  - **Retrieval Augmented Generation (RAG)**: Provides accurate answers by retrieving relevant document chunks.\n  - **Structured Output/JSON Mode**: Delivers answers in a consistent JSON format for clarity and usability.\n  - **Embeddings**: Uses AI embeddings to enable semantic search within documents.\n\n### How It Works\n1. **Upload a PDF**: Students upload study materials (e.g., lecture notes, textbooks).\n2. **Process Documents**: The AI extracts text, splits it into chunks, and creates a vector database for efficient retrieval.\n3. **Ask Questions**: Students ask questions, and StudyGenie retrieves relevant context to provide concise, accurate answers.\n4. **Generate Study Materials**: The AI creates summaries, MCQs, and flashcards to reinforce learning.\n\n### Target Audience\n- Students preparing for exams.\n- Educators creating study resources.\n- Lifelong learners seeking to understand complex texts.\n\nThis notebook demonstrates a fully functional prototype, showcasing how generative AI can revolutionize education. Let‚Äôs dive into the code and see StudyGenie in action! üöÄ","metadata":{}},{"cell_type":"markdown","source":"## Installing Dependencies\n\nThis cell installs the necessary Python packages to power StudyGenie‚Äôs AI capabilities. It‚Äôs a critical first step to ensure the notebook can process PDFs, generate embeddings, perform RAG, and create structured outputs.\n\n**Why It‚Äôs Important**:\n- Installs libraries like `langchain` for RAG and chaining AI tasks.\n- Includes `pypdf` for document understanding (PDF parsing).\n- Sets up `sentence-transformers` and `faiss-cpu` for embeddings and vector search.\n- Adds `ipywidgets` for an interactive user interface.","metadata":{}},{"cell_type":"code","source":"# Install required packages\n!pip install langchain langchain-community langchain-google-genai pypdf sentence-transformers faiss-cpu google-generativeai python-docx ipywidgets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T22:55:17.167004Z","iopub.execute_input":"2025-04-17T22:55:17.167404Z","iopub.status.idle":"2025-04-17T22:55:22.492145Z","shell.execute_reply.started":"2025-04-17T22:55:17.167348Z","shell.execute_reply":"2025-04-17T22:55:22.490895Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\nRequirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.21)\nRequirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.0.10)\nRequirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\nRequirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\nRequirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\nRequirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\nRequirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (8.1.5)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.54)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\nRequirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.16)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\nRequirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.8.1)\nRequirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\nRequirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\nRequirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\nRequirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\nRequirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (1.34.1)\nRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.160.0)\nRequirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.27.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (3.20.3)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.1)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.0)\nRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.1)\nRequirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.2)\nRequirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\nRequirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\nRequirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (4.0.13)\nRequirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.13)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.19.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.67.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\nRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (75.1.0)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\nRequirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\nRequirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.26.2->langchain-community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.26.2->langchain-community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.26.2->langchain-community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.26.2->langchain-community) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.26.2->langchain-community) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.26.2->langchain-community) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\nRequirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\nRequirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\nRequirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.70.0)\nRequirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.48.2)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.13)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.26.2->langchain-community) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.26.2->langchain-community) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3,>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3,>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Importing Libraries and Setting Constants\n\nThis cell imports the required libraries and defines constants used throughout the notebook. It sets the stage for document processing, AI model integration, and vector database management.\n\n**Why It‚Äôs Important**:\n- Imports `langchain` modules for RAG and document processing.\n- Configures the path for the FAISS vector database, which stores document embeddings for retrieval.\n- Ensures all dependencies are loaded to support generative AI capabilities.\n\n**Role in Workflow**:\n- Prepares the environment for document understanding (PDF loading), embeddings (semantic search), and RAG (context-aware answers).","metadata":{}},{"cell_type":"code","source":"# Standard library imports\nimport os                      # Used for handling file paths and environment variables\nimport json                    # For reading and writing JSON data\nfrom pathlib import Path       # Provides an object-oriented interface to handle file paths\nfrom io import BytesIO         # Allows for in-memory binary stream handling\nfrom docx import Document      # Enables parsing and reading of Word (.docx) documents\n\n# LangChain ecosystem imports\nfrom langchain_community.document_loaders import PyPDFLoader         # Loads and extracts text from PDF documents\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter   # Splits large documents into overlapping text chunks for processing\nfrom langchain_community.vectorstores import FAISS                   # Vector store for similarity search based on document embeddings\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings      # Embedding model using Google Gemini for vector generation\nfrom langchain_google_genai import ChatGoogleGenerativeAI            # Chat model interface to interact with Gemini LLM\nfrom langchain.chains import RetrievalQA, LLMChain                   # Pre-built chains for retrieval-augmented generation and language model pipelines\nfrom langchain.prompts import PromptTemplate                         # Allows for structured and reusable prompt creation\n\n# Google Generative AI API setup\nimport google.generativeai as genai                                  # Required for authentication and configuration of Gemini API access\n\n# Path to store or load the FAISS index\nFAISS_INDEX_PATH = \"./faiss_index\"\n","metadata":{"execution":{"iopub.status.busy":"2025-04-18T00:36:06.579547Z","iopub.execute_input":"2025-04-18T00:36:06.580553Z","iopub.status.idle":"2025-04-18T00:36:06.589085Z","shell.execute_reply.started":"2025-04-18T00:36:06.580511Z","shell.execute_reply":"2025-04-18T00:36:06.587651Z"},"trusted":true},"outputs":[{"name":"stdout","text":"The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Configuring the Google API Key\n\nThis cell creates an interactive interface for users to input their Google API key securely. The key is essential for accessing Google‚Äôs generative AI models.\n\n**Why It‚Äôs Important**:\n- Securely configures the AI client, enabling access to embeddings and text generation.\n- Uses `ipywidgets` for a user-friendly interface, enhancing accessibility.\n- Validates the API key to ensure the notebook runs without errors.\n\n**Role in Workflow**:\n- Initializes the generative AI backend, which powers document understanding, embeddings, and RAG.\n\n**Note**: If `ipywidgets` fails to load, ensure it‚Äôs installed (`pip install ipywidgets`) and Jupyter is configured to support widgets (`jupyter nbextension enable --py widgetsnbextension`).","metadata":{}},{"cell_type":"code","source":"# Importing interactive widget libraries for use in a Jupyter Notebook\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\n\n# Create a secure input field for the user to enter their Google API key\napi_key_input = widgets.Password(\n    value='',\n    placeholder='Enter your Google API Key here',\n    description='Google API Key:',\n    disabled=False,\n    style={'description_width': 'initial'},\n    layout=widgets.Layout(width='50%')\n)\n\n# Create a button that will trigger the API configuration process\nconfigure_button = widgets.Button(\n    description='Configure API Key',\n    button_style='info',\n    tooltip='Click to configure the Google AI client with your key'\n)\n\n# Create an output area to show feedback messages\napi_status_output = widgets.Output()\n\n# Define the function that runs when the user clicks the button\ndef on_configure_button_clicked(b):\n    with api_status_output:\n        clear_output()  # Clear previous messages\n        api_key = api_key_input.value\n        if not api_key:\n            print('‚ö†Ô∏è Please enter your Google API Key.')\n            return\n\n        try:\n            print('Configuring Google AI Client...')\n            genai.configure(api_key=api_key)  # Apply the API key to configure access\n            print('‚úÖ Google AI Client Configured Successfully!')\n            api_key_input.disabled = True      # Lock input after success\n            configure_button.disabled = True   # Disable button to prevent changes\n        except Exception as e:\n            print(f'‚ùå Failed to configure Google AI Client: {e}')\n\n# Connect the button to the handler function\nconfigure_button.on_click(on_configure_button_clicked)\n\n# Render the input and button together in the notebook\ndisplay(widgets.VBox([api_key_input, configure_button, api_status_output]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:36:14.078865Z","iopub.execute_input":"2025-04-18T00:36:14.079233Z","iopub.status.idle":"2025-04-18T00:36:14.098076Z","shell.execute_reply.started":"2025-04-18T00:36:14.079210Z","shell.execute_reply":"2025-04-18T00:36:14.097153Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Password(description='Google API Key:', layout=Layout(width='50%'), placeholder='Enter your Goo‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bf448af729149e1891591190df7633d"}},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"## Loading and Processing PDF Documents\n\nThis cell defines functions to load PDF files, split them into manageable chunks, and create a FAISS vector database for retrieval. It leverages **Document Understanding** and **Embeddings** to prepare documents for RAG.\n\n**Why It‚Äôs Important**:\n- `load_pdf`: Extracts text from PDFs, enabling document understanding.\n- `process_documents`: Splits text into chunks for efficient processing and retrieval.\n- `create_vectordb`: Generates embeddings and stores them in a FAISS database for semantic search.\n\n**Role in Workflow**:\n- Converts raw PDFs into a structured format for AI processing.\n- Enables RAG by creating a searchable vector database.","metadata":{}},{"cell_type":"code","source":"def load_pdf(pdf_path):\n    \"\"\"Load a PDF file and return a list of documents\"\"\"\n    try:\n        # Use PyPDFLoader to extract text content from the provided PDF\n        loader = PyPDFLoader(pdf_path)\n        return loader.load()\n    except Exception as e:\n        # Log any issues during loading\n        print(f\"Error loading PDF: {e}\")\n        return None\n\n\ndef process_documents(documents):\n    \"\"\"Process and split the documents into chunks\"\"\"\n    if not documents:\n        return []\n\n    try:\n        # Use a recursive character splitter to divide the text into overlapping chunks\n        # This improves retrieval accuracy in RAG by preserving context\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1000,       # Number of characters per chunk\n            chunk_overlap=200,     # Overlap between chunks to maintain context\n            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Prioritize splitting at logical points\n        )\n        chunks = text_splitter.split_documents(documents)\n        return chunks\n    except Exception as e:\n        # Handle errors in case the document can't be split properly\n        print(f\"Error processing documents: {e}\")\n        return []\n\n\ndef create_vectordb(chunks, api_key):\n    \"\"\"Create and save a FAISS vector database from the document chunks\"\"\"\n    if not chunks:\n        return None\n\n    try:\n        # Create embeddings using Google's Gemini embedding model\n        embeddings = GoogleGenerativeAIEmbeddings(\n            model=\"models/embedding-001\",\n            google_api_key=api_key\n        )\n\n        # Ensure the directory for storing FAISS index exists\n        os.makedirs(FAISS_INDEX_PATH, exist_ok=True)\n\n        # Build the FAISS vector store from the document chunks\n        vectordb = FAISS.from_documents(\n            documents=chunks,\n            embedding=embeddings\n        )\n\n        # Save the vector store locally to disk for reuse\n        vectordb.save_local(folder_path=FAISS_INDEX_PATH)\n        print(f\"Vector database created and saved to {FAISS_INDEX_PATH}\")\n        return vectordb\n    except Exception as e:\n        # Catch and log any error during vector DB creation\n        print(f\"Error creating FAISS vector database: {e}\")\n        return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:36:32.709385Z","iopub.execute_input":"2025-04-18T00:36:32.710268Z","iopub.status.idle":"2025-04-18T00:36:32.718117Z","shell.execute_reply.started":"2025-04-18T00:36:32.710242Z","shell.execute_reply":"2025-04-18T00:36:32.717114Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Setting Up the RAG-Based Q&A System\n\nThis cell configures a **Retrieval Augmented Generation (RAG)** system to answer questions based on the uploaded PDF. It uses a structured prompt to ensure answers are concise and formatted as JSON (**Structured Output**).\n\n**Why It‚Äôs Important**:\n- Combines retrieval (from the FAISS vector database) with generation (using Google‚Äôs Gemini model) to provide accurate, context-aware answers.\n- Uses a structured prompt to enforce JSON output, ensuring consistency and usability.\n- Enhances learning by providing follow-up questions and resource suggestions.\n\n**Role in Workflow**:\n- Enables students to ask questions and receive precise answers grounded in the document.\n- Demonstrates RAG and structured output capabilities, key to the Kaggle competition.","metadata":{}},{"cell_type":"code","source":"def setup_qa_system(vectordb, api_key):\n    \"\"\"Setup a RAG-based Q&A system\"\"\"\n    if not vectordb:\n        return None\n\n    try:\n        # Initialize the Gemini language model (via Vertex AI) for response generation\n        llm = ChatGoogleGenerativeAI(\n            model=\"gemini-1.5-flash\",                 # Fast, cost-efficient Gemini model\n            google_api_key=api_key,                   # Google API key for authentication\n            temperature=0.2,                          # Low randomness for factual answers\n            top_p=0.95,                               # Top-p nucleus sampling\n            max_output_tokens=2048,                   # Max response length\n            convert_system_message_to_human=True      # Converts system prompts for better clarity\n        )\n\n        # Define a detailed prompt template that instructs the model to:\n        # - Provide short, precise explanations\n        # - Suggest follow-up questions\n        # - Recommend external learning resources\n        # - Format everything in clean JSON for structured output\n        template = \"\"\"\n        You are an expert tutor named StudyGenie specializing in explaining complex topics clearly and concisely based on the provided files. Your goal is to help a user understand a specific concept using ONLY the text provided from these documents.\n\n        **Instructions:**\n\n        1. Read the user's {question}.\n        2. Carefully analyze the {context} provided below (this context is extracted from the uploaded documents).\n        3. Generate a concise, clear, and accurate explanation of the {question} based *strictly* on the information within the {context}. Do not add external knowledge or information not present in the documents. Keep the explanation focused and to the point (2-4 sentences).\n        4. Generate 2-3 distinct, relevant follow-up questions that a learner might ask to deepen their understanding of your explanation or the concept itself, based on the document content.\n        5. Suggest 1-2 relevant search terms or types of external resources (e.g., 'search for tutorials on [topic]', 'look for articles explaining [concept]') that could help the user learn more about the topic based on the context.\n        6. Format your entire response *strictly* as a single, valid JSON object. Do NOT include any text before or after the JSON object itself. The JSON object must have these exact keys: \"explanation\" (string), \"follow_up_questions\" (list of strings), and \"potential_resources\" (list of strings, where each string is a suggested search term or resource type).\n\n        **Context:**\n        Context:\n        {context}\n\n        **Concept:**\n        Concept:\n        {question}\n\n        **JSON Output (MUST be only valid JSON):**\n        \"\"\"\n\n        # Wrap the prompt in a LangChain-compatible PromptTemplate\n        PROMPT = PromptTemplate(\n            template=template,\n            input_variables=[\"context\", \"question\"]\n        )\n\n        # Setup a RetrievalQA chain using the vector database and prompt\n        # This enables RAG: retrieves top 5 relevant chunks and passes them into the prompt\n        qa_chain = RetrievalQA.from_chain_type(\n            llm=llm,\n            chain_type=\"stuff\",  # Basic input stuffing; suitable for short contexts\n            retriever=vectordb.as_retriever(search_kwargs={\"k\": 5}),\n            chain_type_kwargs={\"prompt\": PROMPT},\n            return_source_documents=True  # Return source chunks for context transparency\n        )\n\n        return qa_chain\n\n    except Exception as e:\n        # Handle and report initialization errors\n        print(f\"Error setting up Q&A system: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:36:36.041991Z","iopub.execute_input":"2025-04-18T00:36:36.042305Z","iopub.status.idle":"2025-04-18T00:36:36.050303Z","shell.execute_reply.started":"2025-04-18T00:36:36.042286Z","shell.execute_reply":"2025-04-18T00:36:36.049237Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Handling User Questions\n\nThis cell defines a function to process user questions using the RAG system. It retrieves relevant document chunks, generates an answer, and parses the JSON response.\n\n**Why It‚Äôs Important**:\n- Implements the core Q&A functionality, allowing students to ask questions about their study materials.\n- Ensures answers are structured (JSON) and grounded in the document context (RAG).\n- Provides source documents for transparency, enhancing trust in the AI‚Äôs responses.\n\n**Role in Workflow**:\n- Delivers precise, context-aware answers to support student learning.\n- Demonstrates RAG and structured output capabilities for the Kaggle competition.","metadata":{}},{"cell_type":"code","source":"def ask_question(qa_chain, concept_to_explain):\n    \"\"\"Ask a question to the QA system and process its JSON response\"\"\"\n    \n    # Check if the QA system is initialized\n    if not qa_chain:\n        print(\"QA system not initialized. Please upload a PDF first.\")\n        return None, None\n\n    try:\n        # Send the question to the QA chain (RAG + Gemini)\n        result = qa_chain.invoke({\"query\": concept_to_explain})\n        \n        # Extract the JSON string and the source documents\n        answer_json_str = result.get(\"result\")\n        source_docs = result.get(\"source_documents\", [])\n\n        try:\n            # Remove code block formatting if present (e.g., Markdown style)\n            if answer_json_str.startswith(\"```json\\n\"):\n                answer_json_str = answer_json_str[7:]\n            if answer_json_str.endswith(\"\\n```\"):\n                answer_json_str = answer_json_str[:-4]\n            answer_json_str = answer_json_str.strip()\n\n            # Attempt to parse the cleaned string as JSON\n            answer_data = json.loads(answer_json_str)\n\n            # Validate that all required keys are present\n            required_keys = [\"explanation\", \"follow_up_questions\", \"potential_resources\"]\n            if not all(k in answer_data for k in required_keys):\n                print(\"AI response is valid JSON but missing expected keys.\")\n                return None, None\n\n            # Return the structured result and the documents used to generate the answer\n            return answer_data, source_docs\n\n        except json.JSONDecodeError as e:\n            # Handle cases where the model returned invalid JSON\n            print(f\"Failed to parse the response from the AI as JSON. Error: {e}\")\n            return None, None\n\n    except Exception as e:\n        # Catch any general error in the QA pipeline\n        print(f\"Error asking question: {e}\")\n        return None, None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:36:39.311065Z","iopub.execute_input":"2025-04-18T00:36:39.312141Z","iopub.status.idle":"2025-04-18T00:36:39.320260Z","shell.execute_reply.started":"2025-04-18T00:36:39.312111Z","shell.execute_reply":"2025-04-18T00:36:39.319038Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## Generating Study Materials\n\nThis cell defines functions to generate study materials (summaries, MCQs, and flashcards) using the generative AI model. It leverages **Document Understanding** to extract key concepts and **Structured Output** for formatted results.\n\n**Why It‚Äôs Important**:\n- Creates summaries to help students grasp main ideas quickly.\n- Generates MCQs and flashcards to reinforce learning and test understanding.\n- Uses structured prompts to ensure consistent, exam-relevant outputs.\n\n**Role in Workflow**:\n- Enhances study efficiency by providing tailored resources.\n- Demonstrates document understanding and structured output for the Kaggle competition.","metadata":{}},{"cell_type":"code","source":"def create_llm_for_generation(api_key):\n    \"\"\"Creates an LLM instance specifically for generation tasks.\"\"\"\n    try:\n        # Initialize the Gemini model for content generation (e.g., summaries, MCQs, flashcards)\n        llm = ChatGoogleGenerativeAI(\n            model=\"gemini-1.5-pro\",                  # Higher capacity model for richer outputs\n            google_api_key=api_key,\n            temperature=0.3,                         # Low to moderate creativity\n            max_output_tokens=4096,                  # Maximum allowed token length\n            convert_system_message_to_human=True     # Improves response readability\n        )\n        return llm\n    except Exception as e:\n        # Catch and report any issues during model initialization\n        print(f\"Error creating LLM for generation: {e}\")\n        return None\n\n\ndef create_summary_generator(llm):\n    \"\"\"Create a summary generator chain.\"\"\"\n    # Template prompts Gemini to produce an educational summary\n    summary_template = \"\"\"\n    You are an expert educational summarizer. Create a clear, concise summary of the following text.\n    Focus on the main concepts, key points, and critical details.\n\n    TEXT:\n    {text}\n\n    SUMMARY:\n    \"\"\"\n    # Wrap prompt in LangChain's PromptTemplate\n    summary_prompt = PromptTemplate(template=summary_template, input_variables=[\"text\"])\n    \n    # Create an LLMChain for text summarization\n    summary_chain = LLMChain(llm=llm, prompt=summary_prompt)\n    return summary_chain\n\n\ndef create_mcq_generator(llm):\n    \"\"\"Create a multiple-choice question generator chain.\"\"\"\n    # Prompt template instructs the LLM to generate conceptual MCQs based on academic standards\n    mcq_template = \"\"\"\n    You are an expert educator designing exam-applicable multiple-choice questions. Based *only* on the core concepts, key principles, and significant information presented in the following text, create {num_questions} multiple-choice questions.\n\n    **Focus exclusively on understanding and application of the material.** Do NOT ask questions about:\n    - Document metadata (e.g., page numbers, specific section titles unless core to the content)\n    - The source of the information (e.g., \"According to page 5...\")\n    - Trivial details or overly specific examples unless they illustrate a fundamental concept.\n\n    Each question must test conceptual understanding and have 4 options (labeled A, B, C, D) with only one clearly correct answer. Format your response as plain text, following this structure EXACTLY for each question:\n\n    Q[question number]: [Question text focusing on core concepts]\n    A. [Option A]\n    B. [Option B]\n    C. [Option C]\n    D. [Option D]\n    Correct Answer: [Correct option letter, e.g., C]\n    Explanation: [Brief explanation of why the answer is correct, linking back to the core concept]\n\n    (Ensure there is a blank line between each question block)\n\n    TEXT:\n    {text}\n\n    EXAM-APPLICABLE MULTIPLE CHOICE QUESTIONS FOCUSED ON CORE CONCEPTS:\n    \"\"\"\n    # Setup the prompt with dynamic inputs for number of questions and content\n    mcq_prompt = PromptTemplate(\n        template=mcq_template,\n        input_variables=[\"text\", \"num_questions\"]\n    )\n    \n    # Chain that generates MCQs from input text\n    mcq_chain = LLMChain(llm=llm, prompt=mcq_prompt)\n    return mcq_chain\n\n\ndef create_flashcard_generator(llm):\n    \"\"\"Create a flashcard generator chain.\"\"\"\n    # Prompt for generating clean, conceptual flashcards in Q&A format\n    flashcard_template = \"\"\"\n    You are an expert educator. Create {num_cards} flashcards focusing on the CORE CONCEPTS, KEY TERMS, and MAIN IDEAS presented in the following text.\n    Avoid trivial details or overly specific examples unless they are central to understanding a core concept.\n    Format your response as PLAIN TEXT, following this structure EXACTLY for each card:\n\n    Front: [Question, term, or concept]\n    Back: [Answer, definition, or explanation]\n\n    (Ensure there is a blank line between each card block)\n\n    TEXT:\n    {text}\n\n    PLAIN TEXT FLASHCARDS (focus on core concepts):\n    \"\"\"\n    # Define prompt inputs\n    flashcard_prompt = PromptTemplate(\n        template=flashcard_template,\n        input_variables=[\"text\", \"num_cards\"]\n    )\n    \n    # Chain to generate flashcards using the LLM\n    flashcard_chain = LLMChain(llm=llm, prompt=flashcard_prompt)\n    return flashcard_chain\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:36:42.028308Z","iopub.execute_input":"2025-04-18T00:36:42.028677Z","iopub.status.idle":"2025-04-18T00:36:42.038575Z","shell.execute_reply.started":"2025-04-18T00:36:42.028653Z","shell.execute_reply":"2025-04-18T00:36:42.037275Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## Orchestrating Study Material Generation\n\nThis cell combines the summary, MCQ, and flashcard generators to produce comprehensive study materials from document chunks.\n\n**Why It‚Äôs Important**:\n- Integrates multiple generative AI tasks to create a complete study package.\n- Processes document chunks to extract key concepts, ensuring relevance.\n- Supports customizable output (e.g., number of MCQs/flashcards) for flexibility.\n\n**Role in Workflow**:\n- Delivers actionable study resources to enhance learning.\n- Demonstrates document understanding and structured output for the Kaggle competition.","metadata":{}},{"cell_type":"code","source":"def generate_study_materials(chunks, api_key, num_chunks=5, num_mcqs=5, num_flashcards=5):\n    \"\"\"Generate study materials (summary, MCQs, flashcards) from selected chunks.\"\"\"\n    \n    # Check for available document chunks\n    if not chunks:\n        print(\"No document chunks available to generate study materials.\")\n        return None\n\n    # Create a language model instance specifically for generation tasks\n    llm = create_llm_for_generation(api_key)\n    if not llm:\n        return None\n\n    # Initialize the three generator chains using the LLM\n    summary_generator = create_summary_generator(llm)\n    mcq_generator = create_mcq_generator(llm)\n    flashcard_generator = create_flashcard_generator(llm)\n\n    # Select a subset of the chunks to limit token usage and ensure efficiency\n    selected_chunks = chunks[:min(num_chunks, len(chunks))]\n    if not selected_chunks:\n        print(\"Not enough document chunks to generate materials.\")\n        return None\n\n    # Combine the content of the selected chunks into a single input string\n    combined_text = \"\\n\\n\".join([chunk.page_content for chunk in selected_chunks])\n\n    results = {}\n\n    # Generate summary from the text\n    try:\n        print(\"Generating summary...\")\n        summary = summary_generator.invoke({\"text\": combined_text}).get(\"text\", \"Failed to generate summary.\")\n        results[\"summary\"] = summary\n    except Exception as e:\n        print(f\"Error generating summary: {e}\")\n        results[\"summary\"] = \"Error generating summary.\"\n\n    # Generate multiple-choice questions\n    try:\n        print(\"Generating MCQs...\")\n        mcq_response = mcq_generator.invoke({\n            \"text\": combined_text,\n            \"num_questions\": num_mcqs\n        }).get(\"text\")\n        results[\"mcqs_text\"] = mcq_response if mcq_response else \"MCQ generation failed or returned empty.\"\n    except Exception as e:\n        print(f\"Error generating MCQs: {e}\")\n        results[\"mcqs_text\"] = f\"Error during MCQ generation: {e}\"\n\n    # Generate flashcards\n    try:\n        print(\"Generating flashcards...\")\n        flashcard_response = flashcard_generator.invoke({\n            \"text\": combined_text,\n            \"num_cards\": num_flashcards\n        }).get(\"text\")\n        results[\"flashcards_text\"] = flashcard_response if flashcard_response else \"Flashcard generation failed or returned empty.\"\n    except Exception as e:\n        print(f\"Error generating flashcards: {e}\")\n        results[\"flashcards_text\"] = f\"Error during flashcard generation: {e}\"\n\n    return results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:36:46.039693Z","iopub.execute_input":"2025-04-18T00:36:46.040543Z","iopub.status.idle":"2025-04-18T00:36:46.050103Z","shell.execute_reply.started":"2025-04-18T00:36:46.040513Z","shell.execute_reply":"2025-04-18T00:36:46.048815Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## Creating the Interactive Interface\n\nThis cell sets up an interactive interface using `ipywidgets` to allow users to upload PDFs, ask questions, and generate study materials. It integrates all components into a cohesive user experience.\n\n**Why It‚Äôs Important**:\n- Provides a user-friendly interface for students to interact with StudyGenie.\n- Handles file uploads, question inputs, and study material generation dynamically.\n- Displays results clearly, enhancing usability and engagement.\n\n**Role in Workflow**:\n- Ties together document understanding, RAG, and structured output into a practical application.\n- Demonstrates the real-world impact of the project for the Kaggle competition.\n\n**Note**: Ensure `ipywidgets` is properly installed and configured to avoid errors.","metadata":{}},{"cell_type":"code","source":"# File upload widget: Allows the user to upload a PDF file\nfile_upload = widgets.FileUpload(\n    accept='.pdf',  # Accepts only PDF files\n    multiple=False,  # Allows only one file at a time\n    description='Upload PDF:',  # Label for the upload button\n    style={'description_width': 'initial'}  # Adjusts the width of the description label\n)\n\n# Question input widget: Text field for the user to enter their question\nquestion_input = widgets.Text(\n    value='',  # Initial value of the text field\n    placeholder='Enter your question here',  # Placeholder text when the field is empty\n    description='Question:',  # Label for the question input field\n    style={'description_width': 'initial'},  # Adjusts the width of the description label\n    layout=widgets.Layout(width='50%')  # Sets the width of the input field to 50%\n)\n\n# Study material generation controls: Sliders to specify the number of study materials to generate\nnum_mcqs = widgets.IntSlider(\n    value=5,  # Default value for the slider\n    min=1,  # Minimum value\n    max=20,  # Maximum value\n    step=1,  # Step size for slider movement\n    description='Number of MCQs:',  # Label for the MCQ slider\n    style={'description_width': 'initial'}  # Adjusts the width of the description label\n)\n\nnum_flashcards = widgets.IntSlider(\n    value=5,  # Default value for the slider\n    min=1,  # Minimum value\n    max=20,  # Maximum value\n    step=1,  # Step size for slider movement\n    description='Number of Flashcards:',  # Label for the flashcard slider\n    style={'description_width': 'initial'}  # Adjusts the width of the description label\n)\n\n# Output areas: These widgets will display the output for QA and study materials\nqa_output = widgets.Output()\nstudy_materials_output = widgets.Output()\n\n# Global variables to store the state of the system\ncurrent_vectordb = None  # Stores the vector database for QA\ncurrent_qa_chain = None  # Stores the QA chain system\ncurrent_chunks = []  # Stores the chunks of the processed document\n\n# Function to process the uploaded PDF file\ndef process_uploaded_file(change):\n    global current_vectordb, current_qa_chain, current_chunks\n    \n    # Ensure a file is uploaded before processing\n    if not file_upload.value:\n        return\n    \n    with qa_output:\n        clear_output()  # Clear previous output\n        print(\"Processing uploaded PDF...\")\n        \n        # Save the uploaded PDF content\n        pdf_content = file_upload.value[0]['content']\n        pdf_name = file_upload.value[0]['name']\n        \n        with open(pdf_name, 'wb') as f:\n            f.write(pdf_content)\n        \n        # Process the PDF content into documents and chunks\n        documents = load_pdf(pdf_name)\n        if documents:\n            current_chunks = process_documents(documents)\n            if current_chunks:\n                current_vectordb = create_vectordb(current_chunks, api_key_input.value)  # Create vector database\n                if current_vectordb:\n                    current_qa_chain = setup_qa_system(current_vectordb, api_key_input.value)  # Set up the QA system\n                    if current_qa_chain:\n                        print(\"‚úÖ PDF processed successfully! You can now ask questions or generate study materials.\")\n                    else:\n                        print(\"‚ùå Failed to setup Q&A system.\")\n                else:\n                    print(\"‚ùå Failed to create vector database.\")\n            else:\n                print(\"‚ùå Failed to process document chunks.\")\n        else:\n            print(\"‚ùå Failed to load PDF document.\")\n        \n        # Clean up the temporary PDF file after processing\n        os.remove(pdf_name)\n\n# Function to handle asking a question\ndef ask_question_handler(change):\n    # Check if QA system is set up before processing the question\n    if not current_qa_chain:\n        with qa_output:\n            clear_output()\n            print(\"Please upload a PDF first.\")\n        return\n    \n    with qa_output:\n        clear_output()  # Clear previous output\n        print(f\"Question: {question_input.value}\")\n        print(\"\\nGenerating answer...\\n\")\n        \n        # Get answer data and source documents based on the question\n        answer_data, source_docs = ask_question(current_qa_chain, question_input.value)\n        \n        if answer_data:\n            print(\"Explanation:\")\n            print(answer_data[\"explanation\"])\n            \n            print(\"\\nFollow-up Questions:\")\n            for q in answer_data[\"follow_up_questions\"]:\n                print(f\"- {q}\")\n            \n            print(\"\\nPotential Resources:\")\n            for r in answer_data[\"potential_resources\"]:\n                print(f\"- {r}\")\n            \n            if source_docs:\n                print(\"\\nSources:\")\n                for i, doc in enumerate(source_docs):\n                    print(f\"\\nSource {i+1}:\")\n                    print(doc.page_content[:200] + \"...\")\n        else:\n            print(\"Failed to generate an answer.\")\n\n# Function to handle the generation of study materials\ndef generate_study_materials_handler(change):\n    # Ensure that chunks are available before generating study materials\n    if not current_chunks:\n        with study_materials_output:\n            clear_output()\n            print(\"Please upload a PDF first.\")\n        return\n    \n    with study_materials_output:\n        clear_output()  # Clear previous output\n        print(\"Generating study materials...\\n\")\n        \n        # Generate study materials including MCQs and flashcards\n        materials = generate_study_materials(\n            current_chunks,\n            api_key_input.value,\n            num_mcqs=num_mcqs.value,\n            num_flashcards=num_flashcards.value\n        )\n        \n        if materials:\n            print(\"Summary:\")\n            print(materials[\"summary\"])\n            \n            print(\"\\nMultiple Choice Questions:\")\n            print(materials[\"mcqs_text\"])\n            \n            print(\"\\nFlashcards:\")\n            print(materials[\"flashcards_text\"])\n        else:\n            print(\"Failed to generate study materials.\")\n\n# Set up event handlers to trigger functions when values change\nfile_upload.observe(process_uploaded_file, names='value')  # Trigger file processing when file is uploaded\nquestion_input.observe(ask_question_handler, names='value')  # Trigger question handling when user enters a question\n\n# Create buttons for user interactions\nask_button = widgets.Button(\n    description='Ask Question',  # Button label\n    button_style='primary'  # Button style\n)\nask_button.on_click(lambda b: ask_question_handler(None))  # Trigger question handling when button is clicked\n\ngenerate_button = widgets.Button(\n    description='Generate Study Materials',  # Button label\n    button_style='success'  # Button style\n)\ngenerate_button.on_click(lambda b: generate_study_materials_handler(None))  # Trigger study material generation when button is clicked\n\n# Display the interface with all widgets and outputs\nprint(\"üìö StudyGenie: Your AI-Powered Study Assistant\")\nprint(\"\\n1. Configure your Google API Key above\")\nprint(\"2. Upload a PDF document\")\nprint(\"3. Ask questions or generate study materials\")\nprint(\"\\n---\")\n\n# Display widgets in a vertical layout\ndisplay(widgets.VBox([\n    file_upload,  # PDF upload widget\n    widgets.HBox([question_input, ask_button]),  # Question input and button\n    widgets.HBox([num_mcqs, num_flashcards, generate_button]),  # Sliders and generate button\n    qa_output,  # Output for QA results\n    study_materials_output  # Output for study materials\n]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:36:51.584388Z","iopub.execute_input":"2025-04-18T00:36:51.584774Z","iopub.status.idle":"2025-04-18T00:36:51.629487Z","shell.execute_reply.started":"2025-04-18T00:36:51.584748Z","shell.execute_reply":"2025-04-18T00:36:51.628317Z"}},"outputs":[{"name":"stdout","text":"üìö StudyGenie: Your AI-Powered Study Assistant\n\n1. Configure your Google API Key above\n2. Upload a PDF document\n3. Ask questions or generate study materials\n\n---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(FileUpload(value=(), accept='.pdf', description='Upload PDF:'), HBox(children=(Text(value='', d‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a02306d4525e4e58809a244bf32e1c3f"}},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"## üéâ Wrapping Up StudyGenie\n\nThank you for exploring **StudyGenie**, your AI-powered study assistant! This notebook showcases how generative AI can transform learning by processing PDFs, answering questions, and generating study resources like summaries, MCQs, and flashcards. Designed for students and educators, StudyGenie makes studying efficient and engaging.\n\n### Key Achievements\n- **Process PDFs**: Extract and analyze text from academic materials.\n- **Answer Questions**: Deliver precise, context-aware responses.\n- **Generate Resources**: Create tailored summaries, MCQs, and flashcards.\n- **User-Friendly Interface**: Enable easy interaction via file uploads and inputs.\n\n### Generative AI Concepts Used\n- **Document Understanding**: Parse PDFs with `PyPDFLoader`.\n- **Retrieval Augmented Generation (RAG)**: Combine FAISS retrieval with Gemini generation.\n- **Structured Output**: Format answers as JSON for consistency.\n- **Embeddings**: Use Google‚Äôs `embedding-001` for semantic search.\n\n### Get Started\n1. Enter your Google API key in the widget.\n2. Upload a PDF (e.g., lecture notes).\n3. Ask questions or generate study materials.\n4. Fix dependencies if needed (see installation cell).\n\n### Future Potential\n- Add image understanding for diagrams.\n- Support DOCX or handwritten notes.\n- Deploy with MLOps for scalability.","metadata":{}}]}